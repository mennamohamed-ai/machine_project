# ุดุฑุญ ุดุงูู ููุดุฑูุน Tomato Classification

ุดุฑุญ ุชูุตููู ููู ุฌุฒุก ูู ุงูููุฏ ููุง ูุนููู ูููููุฉ ุนููู.

---

## ๐ ููุฑุณ ุงููุญุชููุงุช

1. [HOG (Histogram of Oriented Gradients)](#hog)
2. [Logistic Regression](#logistic-regression)
3. [KMeans Clustering](#kmeans-clustering)
4. [ุงููุฎุฑุฌุงุช ูุงููููุงุช](#ุงููุฎุฑุฌุงุช-ูุงููููุงุช)
5. [ุชุฏูู ุงูุจุฑูุงูุฌ](#ุชุฏูู-ุงูุจุฑูุงูุฌ-step-by-step)

---

## ๐ผ๏ธ HOG (Histogram of Oriented Gradients)

### ูุง ูู HOGุ

**HOG** = ุทุฑููุฉ ูุงุณุชุฎุฑุงุฌ ููุฒุงุช ูู ุงูุตูุฑ ุจูุงุกู ุนูู ุงุชุฌุงู ุงูุญูุงู.

ุงูููุฑุฉ: ุจุฏูุงู ูู ุงุณุชุฎุฏุงู ุงูุตูุฑุฉ ุงูุฃุตููุฉ (64ร64 = 4,096 ุจูุณู)ุ ูุณุชุฎุฑุฌ ูุนูููุงุช ูููุฉ ุนููุง (1,764 ููุฒุฉ).

### ุงููุฑุญูุฉ 1: ุญุณุงุจ ุงูุชุฏุฑุฌุงุช (Gradients)

```python
kx = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])      # Sobel X (ุฃููู)
ky = np.array([[-1,-2,-1],[0,0,0],[1,2,1]])      # Sobel Y (ุนููุฏู)

gx = _conv2d(img, kx)  # ุงูุชุฏุฑุฌุงุช ุงูุฃูููุฉ
gy = _conv2d(img, ky)  # ุงูุชุฏุฑุฌุงุช ุงูุนููุฏูุฉ
```

**ูุง ูุญุฏุซ:**
- ูุญุณุจ ูุฏู ุณุฑุนุฉ ุชุบูุฑ ุงูุฃููุงู ุฃูููุงู ูุนููุฏูุงู
- ุงูุชุบูุฑ ุงูุณุฑูุน = ุญุงูุฉ (edge)
- ุงูุชุบูุฑ ุงูุจุทูุก = ููุทูุฉ ุณูุณุฉ (smooth)

**ูุซุงู:**
```
ุงูุตูุฑุฉ ุงูุฃุตููุฉ:     gx (ุฃููู):      gy (ุนููุฏู):
โโโโโโโโ          โโโโโโโโ         โโโโโโโโ
โโโโโโโโ    โ     โโโโโโโโ    +    โโโโโโโโ
โโโโโโโโ          โโโโโโโโ         โโโโโโโโ
```

### ุงููุฑุญูุฉ 2: ุญุณุงุจ ุงูููุฏุงุฑ ูุงูุงุชุฌุงู

```python
mag = np.hypot(gx, gy)              # ุงูููุฏุงุฑ = โ(gxยฒ + gyยฒ)
ang = np.rad2deg(np.arctan2(gy, gx)) % 180  # ุงูุงุชุฌุงู = atan2(gy, gx)
```

**ุงูุชูุณูุฑ:**
- `mag` (ุงูููุฏุงุฑ): ููุฉ ุงูุญุงูุฉ
  - 0 = ูุง ููุฌุฏ ุญุงูุฉ
  - 255 = ุญุงูุฉ ูููุฉ ุฌุฏุงู
  
- `ang` (ุงูุงุชุฌุงู): ุงุชุฌุงู ุงูุญุงูุฉ ุจุงูุฏุฑุฌุงุช (0-180)
  - 0ยฐ = ุฃููู
  - 90ยฐ = ุนููุฏู
  - 45ยฐ = ูุทุฑู

**ูุซุงู:**
```
ุญุงูุฉ ุฃูููุฉ ูููุฉ: mag=200, ang=0ยฐ
ุญุงูุฉ ุนููุฏูุฉ ูููุฉ: mag=180, ang=90ยฐ
ุญุงูุฉ ูุทุฑูุฉ: mag=150, ang=45ยฐ
```

### ุงููุฑุญูุฉ 3: ุชูุณูู ุงูุตูุฑุฉ ุฅูู ุฎูุงูุง (Cells)

```python
ch, cw = ppc  # pixels per cell = (8, 8)
ncy, ncx = img.shape[0]//ch, img.shape[1]//cw  # ุนุฏุฏ ุงูุฎูุงูุง

# ููุตูุฑุฉ 64ร64 ูุน ุฎูุงูุง 8ร8:
# ncy = 64/8 = 8, ncx = 64/8 = 8
# ุฅุฌูุงูู ุงูุฎูุงูุง = 8ร8 = 64 ุฎููุฉ
```

**ุงูููุฑุฉ:**
- ููุณู ุงูุตูุฑุฉ ุฅูู ุดุจูุฉ 8ร8
- ูู ุฎููุฉ ุจุญุฌู 8ร8 ุจูุณู
- ุงููุชูุฌุฉ: 64 ุฎููุฉ

### ุงููุฑุญูุฉ 4: ุฅูุดุงุก ุฑุณูู ุจูุงููุฉ ููุงุชุฌุงูุงุช (Histograms)

```python
bins = orientations  # 9 (ุงุชุฌุงูุงุช: 0ยฐ, 20ยฐ, 40ยฐ, ..., 160ยฐ)
bin_w = 180/bins  # = 20 ุฏุฑุฌุฉ ููู bin

cell_hist = np.zeros((ncy, ncx, bins))  # 8ร8ร9 = 576 ุฑูู

for cy in range(ncy):
    for cx in range(ncx):
        # ุงุญุตู ุนูู ูู ุงูุจูุณูุงุช ูู ูุฐู ุงูุฎููุฉ
        m = mag[cy*8:(cy+1)*8, cx*8:(cx+1)*8]
        a = ang[cy*8:(cy+1)*8, cx*8:(cx+1)*8]
        
        # ููู ุจูุณู: ุงุถู ููุชู (magnitude) ุฅูู bin ุงูุงุชุฌุงู ุงูููุงุณุจ
        for magnitude, angle in zip(m, a):
            bin_idx = int(angle / 20)  # ุฃู bin ูู 0-8ุ
            cell_hist[cy, cx, bin_idx] += magnitude
```

**ูุซุงู:**
```
ุฎููุฉ ูุงุญุฏุฉ (8ร8 ุจูุณู):

ุงูุงุชุฌุงูุงุช ุงูููุชุดูุฉ:
- 5 ุจูุณูุงุช ุจู 0ยฐ (ุฃููู) ุจููุฉ 150 โ ุฃุถู 150 ุฅูู bin 0
- 3 ุจูุณูุงุช ุจู 90ยฐ (ุนููุฏู) ุจููุฉ 200 โ ุฃุถู 200 ุฅูู bin 4
- 2 ุจูุณู ุจู 45ยฐ ุจููุฉ 100 โ ุฃุถู 100 ุฅูู bin 2

ุงููุชูุฌุฉ: hist = [150, 0, 100, 0, 200, 0, 0, 0, 0]
         (9 ุฃุฑูุงู ุชูุซู ููุฉ ูู ุงุชุฌุงู)
```

### ุงููุฑุญูุฉ 5: ุชุทุจูุน ูู ุฎููุฉ (L2-Hys Normalization)

```python
# ุงููุฑุญูุฉ 5ุฃ: ุชุทุจูุน L2
block = cell_hist[cy:cy+by, cx:cx+bx].ravel()  # ุฎุฐ 2ร2 ุฎูุงูุง (4 ุฎูุงูุง)
block = block / (np.linalg.norm(block) + eps)  # ุงูุณู ุนูู ุงูุทูู

# ูุซุงู:
# block = [100, 50, 200, 150]
# norm = โ(100ยฒ + 50ยฒ + 200ยฒ + 150ยฒ) = โ78500 โ 280
# normalized = [100/280, 50/280, 200/280, 150/280] = [0.36, 0.18, 0.71, 0.54]

# ุงููุฑุญูุฉ 5ุจ: ูุต (Clipping)
block = np.minimum(block, 0.2)  # ุฃู ูููุฉ > 0.2 ุชุตุจุญ 0.2
# [0.36, 0.18, 0.71, 0.54] โ [0.2, 0.18, 0.2, 0.2]

# ุงููุฑุญูุฉ 5ุฌ: ุฅุนุงุฏุฉ ุงูุชุทุจูุน ุจุนุฏ ุงููุต
block = block / (np.linalg.norm(block) + eps)
# [0.2, 0.18, 0.2, 0.2] โ [0.37, 0.33, 0.37, 0.37]
```

**ููุงุฐุง ุงูุชุทุจูุนุ**
- ูุฌุนู ุงูููุฒุงุช ูุงุจูุฉ ููููุงุฑูุฉ
- ูููู ุชุฃุซูุฑ ุงูุฅุถุงุกุฉ
- ูุญุณู ุงูุฃุฏุงุก

### ุงููุฑุญูุฉ 6: ุงููุงุชุฌ ุงูููุงุฆู

```
ุงูุตูุฑุฉ 64ร64
  โ
ุฎูุงูุง 8ร8 (64 ุฎููุฉ)
  โ
ูู ุฎููุฉ = 9 ุงุชุฌุงูุงุช
  โ
ุจูููุงุช 2ร2 (49 ุจููู ูุชุฏุงุฎู)
  โ
ูู ุจููู = 2ร2 ุฎูุงูุง ร 9 ุงุชุฌุงูุงุช = 36 ููุฒุฉ
  โ
ุฅุฌูุงูู ุงูููุฒุงุช = 49 ร 36 = 1,764 โ
```

### ูุซุงู ุจุตุฑู

```
ุงูุตูุฑุฉ ุงูุฃุตููุฉ:            HOG Features:
                          
๐ ุชูููู ุฃุญูุฑ            โฒโถโโผ ุงุชุฌุงูุงุช ูุงุถุญุฉ
                          
64ร64 ุจูุณู              1,764 ุฑูู ููุซู ุงูุจููุฉ
(4,096 ูุนูููุฉ)          (ูุนูููุงุช ูุฑูุฒุฉ)
```

---

## ๐ค Logistic Regression

### ูุง ููุ

ูููุฐุฌ ุชุตููู ูุญุณุจ ุงุญุชูุงููุฉ ุฃู ุชูุชูู ุงูุตูุฑุฉ ูููุฆุฉ ุงูููุฌุจุฉ (ุฑุทุจุฉ).

### ุงููุนุงุฏูุฉ

```
probability = 1 / (1 + e^(-(wยทx + b)))

ุญูุซ:
- x: 1,764 ููุฒุฉ ูู HOG
- w: ุฃูุฒุงู (1,764 ูุฒู)
- b: ุงูุงูุญูุงุฒ (ุฑูู ูุงุญุฏ)
- ุงููุชูุฌุฉ: ุงุญุชูุงููุฉ ุจูู 0 ู 1
```

### ูุซุงู

```
Logistic Regression ูุชุนูู:

ุฅุฐุง ูุงูุช ุงูููุฒุงุช = [0.5, 0.3, 0.8, ...]
ููุงูุช ุงูุฃูุฒุงู = [2.1, 1.5, -0.8, ...]
ูุงูุงูุญูุงุฒ = 0.3

ุงูุญุณุงุจ:
score = 2.1ร0.5 + 1.5ร0.3 + (-0.8)ร0.8 + ... + 0.3 = 1.2
probability = 1 / (1 + e^(-1.2)) = 1 / (1 + 0.3) = 0.77 = 77%

ุงููุชูุฌุฉ: ุงููููุฐุฌ ูููู "ูุฐู ุตูุฑุฉ ุฑุทุจุฉ ุจู 77% ุงุญุชูุงููุฉ"
```

### ููููุฉ ุงูุชุฏุฑูุจ

```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(
    max_iter=1000,      # ุนุฏุฏ ูุฑุงุช ุงูุชูุฑุงุฑ
    solver="liblinear"  # ุฎูุงุฑุฒููุฉ ุงูุชุญุณูู
)
clf.fit(X_train_s, y_train)
```

**ุงูุฎุทูุงุช:**
1. ุงุจุฏุฃ ุจุฃูุฒุงู ุนุดูุงุฆูุฉ
2. ูููู ุตูุฑุฉ ูู ุงูุชุฏุฑูุจ:
   - ุงุญุณุจ ุงูุงุญุชูุงููุฉ
   - ุงุญุณุจ ุงูุฎุทุฃ (ุงููุฑู ุนู ุงูุญูููุฉ)
   - ุญุฏูุซ ุงูุฃูุฒุงู ูุชูููู ุงูุฎุทุฃ
3. ูุฑุฑ 1000 ูุฑุฉ

### ุงูุชูุจุค

```python
y_proba = clf.predict_proba(X_test_s)[:,1]  # ุงุญุชูุงููุฉ ุงููุฆุฉ 1
y_pred = (y_proba >= 0.5).astype(int)       # ุฅุฐุง > 0.5 โ ุฑุทุจ

ูุซุงู:
y_proba = [0.23, 0.91, 0.45, 0.67, ...]
y_pred = [0, 1, 0, 1, ...]
         (ุทุงุฒุฌ, ุฑุทุจ, ุทุงุฒุฌ, ุฑุทุจ, ...)
```

### ุงููุชุงุฆุฌ

```
Accuracy: 85.66%  (ูู 774 ุตูุฑุฉ ุงุฎุชุจุงุฑ)
ROC AUC: 0.8309  (ูุฏุฑุฉ ุฌูุฏุฉ ุนูู ุงูุชูููุฒ)

ุงููุงุฆุฏุฉ: ุงููููุฐุฌ ุจุณูุท ูุณุฑูุน ููุงุจู ููุชูุณูุฑ
```

---

## ๐ค KMeans Clustering

### ูุง ูู KMeansุ

ุฎูุงุฑุฒููุฉ **ุบูุฑ ุฅุดุฑุงููุฉ** (unsupervised) ุชูุณู ุงูุจูุงูุงุช ุฅูู k ูุฌููุนุงุช ุจุฏูู ุงูุญุงุฌุฉ ููุชุณููุงุช.

### ุงูููุฑุฉ ุงูุฃุณุงุณูุฉ

```
ุงููุฏู: ุชูุณูู ุงูุตูุฑ ุฅูู ูุฌููุนุชูู ูุชุดุงุจูุฉ ุฏุงุฎููุงู

ุงูุฎุทูุฉ 1: ุงุฎุชุฑ ูุฑูุฒูู ุนุดูุงุฆูุงู
          โ1      โ2

ุงูุฎุทูุฉ 2: ุงุณูุฏ ูู ููุทุฉ ูุฃูุฑุจ ูุฑูุฒ
          ๐ข๐ข
          ๐ข โ1 ๐ด
             โ2 ๐ด๐ด

ุงูุฎุทูุฉ 3: ุญุฏูุซ ุงููุฑุงูุฒ ุฅูู ูุชูุณุท ูู ูุฌููุนุฉ
          ๐ข๐ข        ๐ข๐ข
          ๐ข  โ1' ๐ด    ๐ข  โ1'
             โ2' ๐ด๐ด          โ2'

ุงูุฎุทูุฉ 4: ูุฑุฑ 2-3 ุญุชู ุงูุชูุงุฑุจ
```

### ูู ูุดุฑูุนูุง

```python
km = KMeans(n_clusters=2, random_state=42)
km.fit(X_train_s)  # ุฏุฑูุจ ุนูู 12,214 ุตูุฑุฉ
```

**ูุง ูุญุฏุซ:**
- ุงูุฎูุงุฑุฒููุฉ ุชูุชุดู ุชููุงุฆูุงู ุฃู ููุงู ูุฌููุนุชุงู ูู ุงูุจูุงูุงุช
- ูุง ูุฎุจุฑูุง ุฃูููุง "ุทุงุฒุฌ" ูุฃูููุง "ุฑุทุจ"
- ูุฏ ุชุฌุฏ cluster 0 = ุทุงุฒุฌ ู cluster 1 = ุฑุทุจุ ุฃู ุงูุนูุณ

### ุงููุดููุฉ: ุชุณููุฉ ุงูู Clusters

```python
# KMeans ุชุนุทููุง labels: 0, 1
# ููููุง ูุง ูุนุฑู ูุงุฐุง ุชูุซู!

val_clusters = km.predict(X_val_s)  # [0, 1, 0, 0, 1, ...]
# ูู cluster 0 ุทุงุฒุฌ ุฃู ุฑุทุจุ

ุงูุญู: ุงุณุชุฎุฏู ูุฌููุนุฉ ุงูุชุญูู (validation set)

mapping = {}
for c in [0, 1]:
    idx = np.where(val_clusters == c)[0]  # ุฃู ุตูุฑ ูู ูุฐุง ุงูู clusterุ
    labels = y_val[idx]                     # ูุง ูู ุงูุชุณููุงุช ุงูุญููููุฉุ
    mapping[c] = Counter(labels).most_common(1)[0][0]  # ุงูุฃูุซุฑ ุดููุนุงู

ูุซุงู:
- cluster 0 ูุญุชูู ุนูู: [ุทุงุฒุฌ, ุทุงุฒุฌ, ุทุงุฒุฌ, ุฑุทุจ, ุทุงุฒุฌ] โ ุงูุฃูุซุฑ = ุทุงุฒุฌ โ mapping[0] = 0
- cluster 1 ูุญุชูู ุนูู: [ุฑุทุจ, ุฑุทุจ, ุฑุทุจ, ุฑุทุจ, ุทุงุฒุฌ] โ ุงูุฃูุซุฑ = ุฑุทุจ โ mapping[1] = 1

ุงููุชูุฌุฉ: mapping = {0: 0, 1: 1}
```

### ุงูุชูุจุค

```python
test_clusters = km.predict(X_test_s)  # [0, 1, 0, 1, ...]
km_pred = np.array([mapping[c] for c in test_clusters])

# ุงุณุชุฎุฏู ุงูู mapping ูุชุญููู cluster labels ุฅูู ุงููุฆุงุช ุงูุญููููุฉ
```

### ุฏุฑุฌุงุช ุงูู ROC

```python
centers = km.cluster_centers_  # ูุฑูุฒ ูู cluster
d0 = np.linalg.norm(X_test_s - centers[0], axis=1)  # ุงููุณุงูุฉ ูู cluster 0
d1 = np.linalg.norm(X_test_s - centers[1], axis=1)  # ุงููุณุงูุฉ ูู cluster 1
km_scores = -(d1)  # ุงุณุชุฎุฏู ุงููุณุงูุฉ ุงูุณุงูุจุฉ ูุฏุฑุฌุฉ

# ุงูููุฑุฉ: ุฅุฐุง ูุงูุช ุงููุณุงูุฉ ูู cluster 1 ูุฑูุจุฉ ุฌุฏุงู โ ุฏุฑุฌุฉ ุนุงููุฉ (ููุฌุจ)
```

### ุงููุชุงุฆุฌ

```
Accuracy: 79.72%  (ุฃูู ูู Logistic ุจู 6%)

ุงูุชูุณูุฑ:
- KMeans ุบูุฑ ุฅุดุฑุงููุ ูุฐุง ูููู ุฃูู ุฏูุฉ ูู ุงูุฅุดุฑุงูู
- ูููู ูุง ูุญุชุงุฌ ููุชุณููุงุช (ููููู ุงูุชุดุงู ุงูุฃููุงุท ุจููุณู)
```

---

## ๐ ุงููุฎุฑุฌุงุช ูุงููููุงุช

### 1. confusion_logistic.png

```python
cm = confusion_matrix(y_test, y_pred)
ax.imshow(cm, cmap="Blues")  # ุฑุณู ูุตูููุฉ ุงูุงูุชุจุงุณ
```

**ูุง ุงูุฐู ูุธูุฑ:**
```
ูุตูููุฉ 2ร2:
           Predicted
           ุทุงุฒุฌ  ุฑุทุจ
Actual ุทุงุฒุฌ  TP   FP
       ุฑุทุจ   FN   TN

ูุซุงู ูู ูุชุงุฆุฌูุง:
           ุทุงุฒุฌ  ุฑุทุจ
ุทุงุฒุฌ       595   22   (ูู 617 ุตูุฑุฉ ุทุงุฒุฌุฉุ ุชููุนูุง ุจุดูู ุตุญูุญ 595)
ุฑุทุจ        95    62   (ูู 157 ุตูุฑุฉ ุฑุทุจุฉุ ุชููุนูุง ุจุดูู ุตุญูุญ 62 ููุท)
```

### 2. roc_logistic.png

```python
fig, ax = plt.subplots()
fpr, tpr, _ = roc_curve(y_test, y_proba)
ax.plot(fpr, tpr)  # ุงูููุญูู
ax.plot([0,1], [0,1], "--")  # ุฎุท ุนุดูุงุฆู
```

**ูุง ุงูุฐู ูุธูุฑ:**
- X ูุญูุฑ: FPR (ูุณุจุฉ ุงูุฎุทุฃ ูู ุงูุณุงูุจ)
- Y ูุญูุฑ: TPR (ูุณุจุฉ ุงูุตุญุฉ ูู ุงูููุฌุจ)
- ุงููุณุงุญุฉ ุชุญุช ุงูููุญูู = AUC = 0.8309

```
ุงูููุญูู ุงูุฃูุถู ูููู ูู ุงูุฒุงููุฉ ุงููุณุฑู ุงูุนููุง
(TPR ุนุงููุ FPR ููุฎูุถ)
```

### 3. learning_curve_logistic.png

```python
fracs = np.linspace(0.1, 1.0, 8)  # [10%, 20%, ..., 100%]
for frac in fracs:
    n = int(frac * len(X_train))  # ุงุณุชุฎุฏู ูุฐู ุงููุณุจุฉ
    # ุฏุฑูุจุ ุงุญุณุจ ุงูุฎุทุฃุ ุณุฌู
```

**ูุง ุงูุฐู ูุธูุฑ:**
- X: ูุณุจุฉ ุงูุจูุงูุงุช ุงููุณุชุฎุฏูุฉ (10% โ 100%)
- Y: ุงูุฎุทุฃ (log_loss)
- Curve 1: ุฎุทุฃ ุงูุชุฏุฑูุจ
- Curve 2: ุฎุทุฃ ุงูุชุญูู

```
ุงูุดูู ุงูุฌูุฏ:
โโ ููุง ุงูุฎุทููู ููุฎูุถุงู
โโ ุงููุณุงูุฉ ุจููููุง ุตุบูุฑุฉ
โโ ูุดูุฑ ุฅูู ุชุนูู ุณููู ุจุฏูู overfitting
```

### 4. confusion_kmeans.png

ููุณ ูุตูููุฉ ุงูุงูุชุจุงุณ ููู ูู KMeans

```
ูุฏ ุชููู ุฃูู ุฏูุฉ ูู Logistic:
           ุทุงุฒุฌ  ุฑุทุจ
ุทุงุฒุฌ       580   37
ุฑุทุจ        123   34
```

### 5. roc_kmeans.png

ููุญูู ROC ูู KMeans

```
ูุฏ ูููู ุฃูู ุงุญููุงุก ูู Logistic
(ูุฃู KMeans ุฃูู ุฏูุฉ)
```

### 6. inertia_vs_k.png

```python
ks = range(1, 7)  # ุฌุฑุจ k = 1, 2, 3, 4, 5, 6
for k in ks:
    km = KMeans(n_clusters=k)
    inertias.append(km.inertia_)  # ูุฌููุน ุงููุณุงูุงุช ุฏุงุฎู ุงูู cluster
```

**ูุง ุงูุฐู ูุธูุฑ:**
```
Inertia (ุงูุฎุทุฃ ุงูุฏุงุฎูู)
  โ
  โ โฒ
  โ  โฒ (ุงูุฎูุงุถ ุณุฑูุน)
  โ   โฒ___
  โ       โฒ (ุงุณุชูุฑุงุฑ)
  โ        โฒ___
  โโโโโโโโโโโโโโโโบ k (ุนุฏุฏ ุงูู clusters)
     1  2  3  4  5  6

ุงูู "elbow" ุนูุฏ k=2 ุฃู k=3
(ุญูุซ ููุฎูุถ ุงูุงูุฎูุงุถ - ููุทุฉ ุงูุงูุญูุงุก)
```

### 7-9. .joblib Files

```python
joblib.dump(clf, "logistic.joblib")          # ุญูุธ ูููุฐุฌ Logistic
joblib.dump(km, "kmeans.joblib")             # ุญูุธ ูููุฐุฌ KMeans
joblib.dump(scaler, "scaler.joblib")         # ุญูุธ ุงููุนูุงุฑู
```

**ุงูุบุฑุถ:**
- ุงุณุชุฎุฏุงููุง ูุงุญูุงู ุจุฏูู ุฅุนุงุฏุฉ ุงูุชุฏุฑูุจ
- ูู ุชุทุจูู ููุจ ุฃู ููุจุงูู
- ูุดุงุฑูุชูุง ูุน ุงูุขุฎุฑูู

**ุงูุงุณุชุฎุฏุงู:**
```python
from sklearn import joblib

# ุชุญููู ุงูููุงุฐุฌ ุงููุญููุธุฉ
clf = joblib.load("logistic.joblib")
scaler = joblib.load("scaler.joblib")

# ุงุณุชุฎุฏุงู ุนูู ุตูุฑ ุฌุฏูุฏุฉ
new_image = load_image("tomato.jpg")
hog_features = compute_hog(new_image)
scaled_features = scaler.transform([hog_features])
prediction = clf.predict(scaled_features)
```

---

## ๐ ุชุฏูู ุงูุจุฑูุงูุฌ Step by Step

### ุงููุฑุญูุฉ 1: ุงูุชุญุถูุฑ

```python
1. ุงุณุชูุฑุงุฏ ุงูููุชุจุงุช
2. ุชุนุฑูู ุงูุฅุนุฏุงุฏุงุช (ูุณุงุฑุงุชุ ุฃุญุฌุงูุ ูุนุงููุงุช)
3. ุฅูุดุงุก ูุฌูุฏ ุงูุฎุฑุฌุงุช
```

### ุงููุฑุญูุฉ 2: ุชุญููู ุงูุจูุงูุงุช

```python
train = load_yolo_dataset("train/")  # 12,214 ุตูุฑุฉ + ุชุณููุงุช
val = load_yolo_dataset("val/")      # 1,530 ุตูุฑุฉ
test = load_yolo_dataset("test/")    # 774 ุตูุฑุฉ

# ูู ุตูุฑุฉ ููุง ุชุณููุฉ: 0 (ุทุงุฒุฌ) ุฃู 1 (ุฑุทุจ)
```

### ุงููุฑุญูุฉ 3: ุงุณุชุฎุฑุงุฌ ููุฒุงุช HOG

```python
X_train, y_train = load_and_extract(train)
# ููู ุตูุฑุฉ:
#   1. ุงูุชุญ ุงูุตูุฑุฉ ูู ุงููุฑุต
#   2. ุญูู ุฅูู ุฑูุงุฏู
#   3. ุบููุฑ ุงูุญุฌู ุฅูู 64ร64
#   4. ุงุณุชุฎุฑุฌ HOG (1,764 ููุฒุฉ)
#   5. ุฎุฒูู ุงูููุฒุงุช ูุงูุชุณููุฉ

ุงููุชูุฌุฉ:
X_train.shape = (12214, 1764)  # 12,214 ุตูุฑุฉุ 1,764 ููุฒุฉ
y_train.shape = (12214,)       # 12,214 ุชุณููุฉ
```

### ุงููุฑุญูุฉ 4: ูุนุงูุฑุฉ ุงูุจูุงูุงุช

```python
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_val_s = scaler.transform(X_val)
X_test_s = scaler.transform(X_test)

# ูุฌุนู ูู ููุฒุฉ: ูุชูุณุท = 0ุ ุงูุญุฑุงู ูุนูุงุฑู = 1
```

### ุงููุฑุญูุฉ 5: ุชุฏุฑูุจ Logistic Regression

```python
clf = LogisticRegression(max_iter=1000, solver="liblinear")
clf.fit(X_train_s, y_train)
# ูุชุนูู 1,764 ูุฒู + 1 ุงูุญูุงุฒ
```

### ุงููุฑุญูุฉ 6: ุงูุชูุจุค ูุงูุชูููู (Logistic)

```python
y_proba = clf.predict_proba(X_test_s)[:,1]  # ุงุญุชูุงููุฉ
y_pred = (y_proba >= 0.5).astype(int)       # ุชุตููู (0 ุฃู 1)

# ุงุญุณุจ:
accuracy = accuracy_score(y_test, y_pred)   # 85.66%
cm = confusion_matrix(y_test, y_pred)       # ุงูุฃุฎุทุงุก ุจุงูุชูุตูู
fpr, tpr, _ = roc_curve(y_test, y_proba)    # ููุญูู ROC
auc_score = auc(fpr, tpr)                   # 0.8309
```

### ุงููุฑุญูุฉ 7: ุฑุณู ุงูุฑุณูู ุงูุจูุงููุฉ (Logistic)

```python
# ุฑุณู ูุตูููุฉ ุงูุงูุชุจุงุณ
fig, ax = plt.subplots()
ax.imshow(cm, cmap="Blues")
save_plot(fig, "confusion_logistic.png")

# ุฑุณู ROC Curve
fig, ax = plt.subplots()
ax.plot(fpr, tpr)
ax.plot([0,1], [0,1], "--")
save_plot(fig, "roc_logistic.png")

# ููุญูู ุงูุชุนูู
# (ููุณ ุงูุฎุทูุงุช ูุน ุฃุญุฌุงู ูุฎุชููุฉ ูู ุงูุจูุงูุงุช)
```

### ุงููุฑุญูุฉ 8: ุชุฏุฑูุจ KMeans

```python
km = KMeans(n_clusters=2, random_state=42)
km.fit(X_train_s)
# ููุชุดู ูุฑูุฒูู ุชููุงุฆูุงู
```

### ุงููุฑุญูุฉ 9: ุชุนููู ุงูู Clusters

```python
val_clusters = km.predict(X_val_s)
mapping = {}
for c in [0, 1]:
    idx = np.where(val_clusters == c)[0]
    mapping[c] = Counter(y_val[idx]).most_common(1)[0][0]
# ุงูุขู ูุนุฑู ูุงุฐุง ููุซู ูู cluster
```

### ุงููุฑุญูุฉ 10: ุงูุชูุจุค ูุงูุชูููู (KMeans)

```python
test_clusters = km.predict(X_test_s)
km_pred = np.array([mapping[c] for c in test_clusters])

accuracy = accuracy_score(y_test, km_pred)  # 79.72%
cm_k = confusion_matrix(y_test, km_pred)
```

### ุงููุฑุญูุฉ 11: ุฑุณู ุงูุฑุณูู ุงูุจูุงููุฉ (KMeans)

```python
# Confusion Matrix
# ROC Curve
# Inertia vs K
```

### ุงููุฑุญูุฉ 12: ุญูุธ ุงูููุงุฐุฌ

```python
joblib.dump(clf, "logistic.joblib")
joblib.dump(km, "kmeans.joblib")
joblib.dump(scaler, "scaler.joblib")
```

### ุงููุฑุญูุฉ 13: ุฅูุดุงุก ุงูุชูุฑูุฑ

```python
report = f"""
# Tomato Classification Report

Dataset: {total} ุตูุฑุฉ
Logistic: {acc_lr}%
KMeans: {acc_km}%
...
"""

with open("project_report.md", "w") as f:
    f.write(report)
```

---

## ๐ ููุฎุต ุงูุนูููุฉ

```
ุตูุฑ ุงูุทูุงุทู (64ร64)
        โ
    ุงุณุชุฎุฑุงุฌ HOG
        โ
    ููุฒุงุช (1,764)
        โ
    ูุนุงูุฑุฉ ุงูุจูุงูุงุช
        โ
    โโโโโโโโโโโโโโโโโ
    โ       โ       โ
ุชุฏุฑูุจ   ุชุญูู  ุงุฎุชุจุงุฑ
    โ       โ       โ
    โโโโโโโโโโโโโโโโโ
        โ
    โโโโโโโโโโโโโโโ
    โ   ูููุฐุฌ     โ
    โ  Logistic   โ
    โโโโโโโโโโโโโโโ
        โ
    ุงูุชูุจุค ูุงูุชูููู
        โ
    ุฑุณูู ุจูุงููุฉ
        โ
    ุญูุธ ุงููููุฐุฌ
```

---

## ๐ฏ ููุฎุต ุงููุชุงุฆุฌ

| ุฌุงูุจ | Logistic | KMeans |
|---|---|---|
| **Accuracy** | 85.66% | 79.72% |
| **ROC AUC** | 0.8309 | - |
| **ุงูููุน** | ุฅุดุฑุงูู | ุบูุฑ ุฅุดุฑุงูู |
| **ุงูุณุฑุนุฉ** | ุณุฑูุน | ุณุฑูุน |
| **ุงูุชูุณูุฑ** | ุฌูุฏ | ุฌูุฏ |
| **ุงูุฃุฏุงุก** | ุฃูุถู | ุฃูู ููู ูุจูู |

**ุงูุฎูุงุตุฉ:**
- Logistic Regression ุฃูุถู ููุชุตููู ุงูุฏููู
- KMeans ูููุฏ ูุงูุชุดุงู ุงูุฃููุงุท ุงูุชููุงุฆูุฉ
- ุงูุงุซูุงู ุณุฑูุนุงู ูุนูููุงู

---

**ุขุฎุฑ ุชุญุฏูุซ:** ุฏูุณูุจุฑ 2025
